{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL - B\n",
    "\n",
    "##  using separate models for user embedding and exercise recommendation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and preprocess data\n",
    "# users_df = pd.read_csv('users.csv')\n",
    "# utterances_df = pd.read_csv('utterances.csv')\n",
    "# errors_df = pd.read_csv('errors.csv')\n",
    "# exercises_df = pd.read_csv('exercises.csv')\n",
    "# user_progress_df = pd.read_csv('user_progress.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess categorical data\n",
    "# le_country = LabelEncoder()\n",
    "# le_age_band = LabelEncoder()\n",
    "# le_proficiency = LabelEncoder()\n",
    "# le_cefr = LabelEncoder()\n",
    "\n",
    "# users_df['country_encoded'] = le_country.fit_transform(users_df['country'])\n",
    "# users_df['age_band_encoded'] = le_age_band.fit_transform(users_df['age_band'])\n",
    "# users_df['proficiency_level_encoded'] = le_proficiency.fit_transform(users_df['proficiency_level'])\n",
    "# users_df['cefr_level_encoded'] = le_cefr.fit_transform(users_df['cefr_level'])\n",
    "\n",
    "# # Normalize numerical data\n",
    "# scaler = StandardScaler()\n",
    "# numerical_columns = ['overall_progress', 'total_practice_time', 'exercises_completed', 'daily_active_days', \n",
    "#                      'weekly_active_days', 'total_sessions', 'avg_session_duration', 'adaptive_difficulty', \n",
    "#                      'engagement_score']\n",
    "# users_df[numerical_columns] = scaler.fit_transform(users_df[numerical_columns])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dictionary to store user embeddings\n",
    "# user_embeddings = {}\n",
    "\n",
    "# # Define the User Embedding Model--CAN BE MODIFIED\n",
    "# class UserEmbeddingModel(nn.Module):\n",
    "#     def __init__(self, num_countries, num_age_bands, num_proficiency_levels, num_cefr_levels, embedding_dim=32):\n",
    "#         super(UserEmbeddingModel, self).__init__()\n",
    "#         self.country_embedding = nn.Embedding(num_countries, embedding_dim)\n",
    "#         self.age_band_embedding = nn.Embedding(num_age_bands, embedding_dim)\n",
    "#         self.proficiency_embedding = nn.Embedding(num_proficiency_levels, embedding_dim)\n",
    "#         self.cefr_embedding = nn.Embedding(num_cefr_levels, embedding_dim)\n",
    "#         self.fc = nn.Linear(embedding_dim * 4 + len(numerical_columns), embedding_dim)\n",
    "\n",
    "#     def forward(self, country, age_band, proficiency, cefr, numerical_features):\n",
    "#         country_emb = self.country_embedding(country)\n",
    "#         age_band_emb = self.age_band_embedding(age_band)\n",
    "#         proficiency_emb = self.proficiency_embedding(proficiency)\n",
    "#         cefr_emb = self.cefr_embedding(cefr)\n",
    "#         combined = torch.cat((country_emb, age_band_emb, proficiency_emb, cefr_emb, numerical_features), dim=1)\n",
    "#         return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7267507314682007\n",
      "Epoch 2, Loss: 1.5114926099777222\n",
      "Epoch 3, Loss: 1.0113203525543213\n",
      "Epoch 4, Loss: 0.9759833812713623\n",
      "Epoch 5, Loss: 0.6415071487426758\n",
      "Epoch 6, Loss: 0.9812778234481812\n",
      "Epoch 7, Loss: 1.2374804019927979\n",
      "Epoch 8, Loss: 0.9554104804992676\n",
      "Epoch 9, Loss: 0.860277533531189\n",
      "Epoch 10, Loss: 1.232203722000122\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# num_countries = len(le_country.classes_)\n",
    "# num_age_bands = len(le_age_band.classes_)\n",
    "# num_proficiency_levels = len(le_proficiency.classes_)\n",
    "# num_cefr_levels = len(le_cefr.classes_)\n",
    "\n",
    "# user_embedding_model = UserEmbeddingModel(num_countries, num_age_bands, num_proficiency_levels, num_cefr_levels)\n",
    "# optimizer = optim.AdamW(user_embedding_model.parameters())\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Train the embedding Model (simplified for brevity)\n",
    "# for epoch in range(10):\n",
    "#     for _, user in users_df.iterrows():\n",
    "#         country = torch.tensor([user['country_encoded']])\n",
    "#         age_band = torch.tensor([user['age_band_encoded']])\n",
    "#         proficiency = torch.tensor([user['proficiency_level_encoded']])\n",
    "#         cefr = torch.tensor([user['cefr_level_encoded']])\n",
    "#         numerical = torch.tensor(user[numerical_columns].astype(np.float32).values).float().unsqueeze(0)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         embedding = user_embedding_model(country, age_band, proficiency, cefr, numerical)\n",
    "#         loss = criterion(embedding, torch.randn_like(embedding))  # Simplified loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "#     # BAD LOSS VALUES BECAUSE OF RANDOM TARGETS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate user embeddings\n",
    "# for _, user in users_df.iterrows():\n",
    "#     country = torch.tensor([user['country_encoded']])\n",
    "#     age_band = torch.tensor([user['age_band_encoded']])\n",
    "#     proficiency = torch.tensor([user['proficiency_level_encoded']])\n",
    "#     cefr = torch.tensor([user['cefr_level_encoded']])\n",
    "#     numerical = torch.tensor(user[numerical_columns].astype(np.float32).values).float().unsqueeze(0)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         embedding = user_embedding_model(country, age_band, proficiency, cefr, numerical)\n",
    "#     user_embeddings[user['user_id']] = embedding.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #Define the Exercise Recommendation Model\n",
    "# class ExerciseRecommendationModel(nn.Module):\n",
    "#     def __init__(self, user_embedding_dim, hidden_dim, num_categories):\n",
    "#         super(ExerciseRecommendationModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(user_embedding_dim + 4, hidden_dim, batch_first=True)\n",
    "#         self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, num_categories)\n",
    "\n",
    "#     def forward(self, user_embedding, error_history):\n",
    "#         lstm_out, _ = self.lstm(torch.cat((user_embedding.unsqueeze(1).repeat(1, error_history.size(1), 1), error_history), dim=2))\n",
    "#         attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "#         return self.fc(attn_out[:, -1, :])\n",
    "\n",
    "# # Prepare data for the Exercise Recommendation Model\n",
    "# def prepare_error_history(user_id, timestamp, window_size=10):\n",
    "#     user_errors = errors_df[(errors_df['user_id'] == user_id) & (errors_df['timestamp'] < timestamp)].sort_values('timestamp', ascending=False).head(window_size)\n",
    "#     error_history = np.zeros((window_size, 4))\n",
    "#     for i, (_, error) in enumerate(user_errors.iterrows()):\n",
    "#         category_index = ['grammar', 'vocabulary', 'pronunciation', 'fluency'].index(error['category'])\n",
    "#         error_history[i, category_index] = error['severity']\n",
    "#     return error_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3223129510879517\n",
      "Epoch 2, Loss: 1.3347889184951782\n",
      "Epoch 3, Loss: 1.3286904096603394\n",
      "Epoch 4, Loss: 1.3279350996017456\n",
      "Epoch 5, Loss: 1.3246467113494873\n",
      "Epoch 6, Loss: 1.3242452144622803\n",
      "Epoch 7, Loss: 1.3075422048568726\n",
      "Epoch 8, Loss: 1.2514656782150269\n",
      "Epoch 9, Loss: 1.2273412942886353\n",
      "Epoch 10, Loss: 1.156772255897522\n"
     ]
    }
   ],
   "source": [
    "# # Initialize and train the Exercise Recommendation Model\n",
    "# num_categories = 4  # grammar, vocabulary, pronunciation, fluency\n",
    "# recommendation_model = ExerciseRecommendationModel(32, 64, num_categories)\n",
    "# optimizer = optim.Adam(recommendation_model.parameters())\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Simplified training loop\n",
    "# for epoch in range(10):\n",
    "#     for _, exercise in exercises_df.iterrows():\n",
    "#         user_id = exercise['user_id']\n",
    "#         timestamp = exercise['timestamp']\n",
    "#         user_embedding = torch.tensor(user_embeddings[user_id]).float().unsqueeze(0)\n",
    "#         error_history = torch.tensor(prepare_error_history(user_id, timestamp)).float().unsqueeze(0)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         prediction = recommendation_model(user_embedding, error_history)\n",
    "#         target = torch.tensor([['grammar', 'vocabulary', 'pronunciation', 'fluency'].index(exercise['category'])])\n",
    "#         loss = criterion(prediction, target)\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(recommendation_model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "#     # BAD LOSS VALUES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def recommend_exercise_category(user_id, timestamp):\n",
    "#     user_embedding = torch.tensor(user_embeddings[user_id]).float().unsqueeze(0)\n",
    "#     error_history = torch.tensor(prepare_error_history(user_id, timestamp)).float().unsqueeze(0)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         prediction = recommendation_model(user_embedding, error_history)\n",
    "    \n",
    "#     category_index = prediction.argmax().item()\n",
    "#     return ['grammar', 'vocabulary', 'pronunciation', 'fluency'][category_index]\n",
    "\n",
    "# # Content and Format Recommendation--CAN BE MODIFIED\n",
    "# def recommend_content_and_format(user_id, category):\n",
    "#     user = users_df[users_df['user_id'] == user_id].iloc[0]\n",
    "#     user_interests = user['interests'].split(', ')\n",
    "#     user_country = user['country']\n",
    "    \n",
    "#     # Simple logic for content theme based on user interests and country\n",
    "#     content_theme = np.random.choice(user_interests)\n",
    "    \n",
    "#     # Simple logic for exercise format - LABELS CAN BE MODIFIED\n",
    "#     formats = [\"fill-in-the-blank\", \"multiple-choice\", \"audio-recording\", \"sentence-formation\"]\n",
    "#     format_weights = [0.3, 0.3, 0.2, 0.2]\n",
    "#     if category == 'pronunciation':\n",
    "#         format_weights = [0.1, 0.1, 0.7, 0.1]\n",
    "#     elif category == 'grammar':\n",
    "#         format_weights = [0.4, 0.4, 0.1, 0.1]\n",
    "    \n",
    "#     exercise_format = np.random.choice(formats, p=format_weights)\n",
    "    \n",
    "#     return content_theme, exercise_format\n",
    "\n",
    "# # Main recommendation function\n",
    "# def recommend_exercise(user_id, timestamp):\n",
    "#     category = recommend_exercise_category(user_id, timestamp)\n",
    "#     content_theme, exercise_format = recommend_content_and_format(user_id, category)\n",
    "    \n",
    "#     return {\n",
    "#         'category': category,\n",
    "#         'content_theme': content_theme,\n",
    "#         'format': exercise_format\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended exercise for user e0fa472b-9738-4814-b366-5e0f6f668547:\n",
      "{'category': 'fluency', 'content_theme': \"'music'\", 'format': 'audio-recording'}\n",
      "Simulating session for user 6e43f566-f99a-40dd-a246-4c63ac6ee7af\n",
      "\n",
      "Utterance 1\n",
      "Recommending exercise:\n",
      "{'category': 'fluency', 'content_theme': \"['literature'\", 'format': 'multiple-choice'}\n",
      "Simulated performance score: 0.99\n",
      "\n",
      "Utterance 2\n",
      "No exercise recommended for this utterance.\n",
      "\n",
      "Utterance 3\n",
      "Recommending exercise:\n",
      "{'category': 'fluency', 'content_theme': \"['literature'\", 'format': 'multiple-choice'}\n",
      "Simulated performance score: 0.78\n",
      "\n",
      "Utterance 4\n",
      "Recommending exercise:\n",
      "{'category': 'fluency', 'content_theme': \"'anime']\", 'format': 'fill-in-the-blank'}\n",
      "Simulated performance score: 0.95\n",
      "\n",
      "Utterance 5\n",
      "Recommending exercise:\n",
      "{'category': 'fluency', 'content_theme': \"['literature'\", 'format': 'fill-in-the-blank'}\n",
      "Simulated performance score: 0.59\n",
      "\n",
      "Utterance 6\n",
      "Recommending exercise:\n",
      "{'category': 'fluency', 'content_theme': \"['literature'\", 'format': 'fill-in-the-blank'}\n",
      "Simulated performance score: 0.64\n",
      "\n",
      "Utterance 7\n",
      "No exercise recommended for this utterance.\n",
      "\n",
      "Utterance 8\n",
      "No exercise recommended for this utterance.\n",
      "\n",
      "Utterance 9\n",
      "No exercise recommended for this utterance.\n",
      "\n",
      "Utterance 10\n",
      "No exercise recommended for this utterance.\n"
     ]
    }
   ],
   "source": [
    "# # Example usage\n",
    "\n",
    "# errors_df['timestamp'] = pd.to_datetime(errors_df['timestamp'])\n",
    "\n",
    "# user_id = users_df['user_id'].iloc[0]\n",
    "# timestamp = pd.Timestamp.now()\n",
    "# recommendation = recommend_exercise(user_id, timestamp)\n",
    "# print(f\"Recommended exercise for user {user_id}:\")\n",
    "# print(recommendation)\n",
    "\n",
    "# # Function to update user embedding after completing an exercise\n",
    "# def update_user_embedding(user_id, exercise_performance):\n",
    "#     # this is a simplified update\n",
    "#     current_embedding = user_embeddings[user_id]\n",
    "#     performance_factor = exercise_performance['performance_score']\n",
    "#     updated_embedding = current_embedding * (1 + 0.1 * performance_factor)\n",
    "#     user_embeddings[user_id] = updated_embedding\n",
    "\n",
    "# #simualtion\n",
    "# def simulate_user_session(user_id, num_utterances=10):\n",
    "#     print(f\"Simulating session for user {user_id}\")\n",
    "#     for i in range(num_utterances):\n",
    "#         timestamp = pd.Timestamp.now() + pd.Timedelta(minutes=i*10)\n",
    "        \n",
    "#         # Simulate user utterance (you would replace this with actual user input in a real system)\n",
    "#         print(f\"\\nUtterance {i+1}\")\n",
    "        \n",
    "#         # Decide whether to show an exercise (simplified logic as this algorithm was in place)\n",
    "#         if np.random.random() < 0.6:  # 60% chance of showing an exercise\n",
    "#             recommendation = recommend_exercise(user_id, timestamp)\n",
    "#             print(\"Recommending exercise:\")\n",
    "#             print(recommendation)\n",
    "            \n",
    "           \n",
    "#             performance_score = np.random.uniform(0.5, 1.0)\n",
    "#             print(f\"Simulated performance score: {performance_score:.2f}\")\n",
    "            \n",
    "           \n",
    "#             update_user_embedding(user_id, {'performance_score': performance_score})\n",
    "#         else:\n",
    "#             print(\"No exercise recommended for this utterance.\")\n",
    "\n",
    "# # Run simulation \n",
    "# random_user_id = np.random.choice(users_df['user_id'])\n",
    "# simulate_user_session(random_user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MODEL with attention mechanisms for processing utterance history.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# users_df = pd.read_csv('users.csv')\n",
    "# utterances_df = pd.read_csv('utterances.csv')\n",
    "# errors_df = pd.read_csv('errors.csv')\n",
    "# exercises_df = pd.read_csv('exercises.csv')\n",
    "# user_progress_df = pd.read_csv('user_progress.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def preprocess_user_data(df):\n",
    "#     le = LabelEncoder()\n",
    "#     cat_columns = ['country', 'age_band', 'proficiency_level', 'cefr_level', 'current_focus']\n",
    "#     for col in cat_columns:\n",
    "#         df[col] = le.fit_transform(df[col])\n",
    "    \n",
    "#     # Handle list columns\n",
    "#     df['interests'] = df['interests'].apply(eval)\n",
    "#     df['personalized_goals'] = df['personalized_goals'].apply(eval)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     num_columns = ['overall_progress', 'total_practice_time', 'exercises_completed', 'daily_active_days',\n",
    "#                    'weekly_active_days', 'total_sessions', 'avg_session_duration', 'adaptive_difficulty',\n",
    "#                    'engagement_score']\n",
    "#     df[num_columns] = scaler.fit_transform(df[num_columns])\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def preprocess_utterance_data(df):\n",
    "#     scaler = StandardScaler()\n",
    "#     num_columns = ['grammar_overall_score', 'grammar_error_count', 'vocabulary_overall_score', 'unique_words_count',\n",
    "#                    'advanced_words_count', 'lexical_density', 'pronunciation_overall_score', 'phoneme_error_count',\n",
    "#                    'intonation_score', 'rhythm_score', 'accent_similarity', 'fluency_overall_score', 'words_per_minute',\n",
    "#                    'filler_word_count', 'pause_count', 'avg_pause_duration', 'longest_fluent_segment', 'coherence_score',\n",
    "#                    'topic_relevance_score']\n",
    "#     df[num_columns] = scaler.fit_transform(df[num_columns])\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def preprocess_exercise_data(df):\n",
    "#     le = LabelEncoder()\n",
    "#     cat_columns = ['category', 'subcategory', 'format', 'content_theme']\n",
    "#     for col in cat_columns:\n",
    "#         df[col] = le.fit_transform(df[col])\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     num_columns = ['difficulty', 'performance_score', 'time_taken']\n",
    "#     df[num_columns] = scaler.fit_transform(df[num_columns])\n",
    "#     return df\n",
    "\n",
    "\n",
    "# users_df = preprocess_user_data(users_df)\n",
    "# utterances_df = preprocess_utterance_data(utterances_df)\n",
    "# exercises_df = preprocess_exercise_data(exercises_df)\n",
    "\n",
    "\n",
    "# def create_user_features(user):\n",
    "#     return torch.tensor([\n",
    "#         user['country'], user['age_band'], user['proficiency_level'], user['cefr_level'],\n",
    "#         user['overall_progress'], user['total_practice_time'], user['exercises_completed'],\n",
    "#         user['daily_active_days'], user['weekly_active_days'], user['total_sessions'],\n",
    "#         user['avg_session_duration'], user['adaptive_difficulty'], user['engagement_score']\n",
    "#     ], dtype=torch.float32)\n",
    "\n",
    "# def create_utterance_features(utterance):\n",
    "#     return torch.tensor([\n",
    "#         utterance['grammar_overall_score'], utterance['grammar_error_count'],\n",
    "#         utterance['vocabulary_overall_score'], utterance['unique_words_count'],\n",
    "#         utterance['advanced_words_count'], utterance['lexical_density'],\n",
    "#         utterance['pronunciation_overall_score'], utterance['phoneme_error_count'],\n",
    "#         utterance['intonation_score'], utterance['rhythm_score'], utterance['accent_similarity'],\n",
    "#         utterance['fluency_overall_score'], utterance['words_per_minute'],\n",
    "#         utterance['filler_word_count'], utterance['pause_count'], utterance['avg_pause_duration'],\n",
    "#         utterance['longest_fluent_segment'], utterance['coherence_score'],\n",
    "#         utterance['topic_relevance_score']\n",
    "#     ], dtype=torch.float32)\n",
    "\n",
    "# def create_exercise_features(exercise):\n",
    "#     return torch.tensor([\n",
    "#         exercise['category'], exercise['subcategory'], exercise['format'],\n",
    "#         exercise['difficulty'], exercise['performance_score'], exercise['time_taken']\n",
    "#     ], dtype=torch.float32)\n",
    "\n",
    "# # Create a dataset for training\n",
    "# class ExerciseRecommendationDataset(Dataset):\n",
    "#     def __init__(self, users, utterances, exercises, max_utterances=10):\n",
    "#         self.users = users\n",
    "#         self.utterances = utterances\n",
    "#         self.exercises = exercises\n",
    "#         self.max_utterances = max_utterances\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.exercises)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         exercise = self.exercises.iloc[idx]\n",
    "#         user_id = exercise['user_id']\n",
    "#         user = self.users[self.users['user_id'] == user_id].iloc[0]\n",
    "#         user_features = create_user_features(user)\n",
    "        \n",
    "#         user_utterances = self.utterances[self.utterances['user_id'] == user_id].tail(self.max_utterances)\n",
    "#         utterance_features = torch.stack([create_utterance_features(u) for _, u in user_utterances.iterrows()])\n",
    "        \n",
    "#         # Pad utterance sequence if necessary\n",
    "#         if len(utterance_features) < self.max_utterances:\n",
    "#             padding = torch.zeros((self.max_utterances - len(utterance_features), utterance_features.size(1)))\n",
    "#             utterance_features = torch.cat([utterance_features, padding], dim=0)\n",
    "        \n",
    "#         exercise_features = create_exercise_features(exercise)\n",
    "        \n",
    "#         return user_features, utterance_features, exercise_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model architecture\n",
    "# class ExerciseRecommendationModel(nn.Module):\n",
    "#     def __init__(self, user_dim, utterance_dim, exercise_dim, hidden_dim):\n",
    "#         super(ExerciseRecommendationModel, self).__init__()\n",
    "#         self.user_embedding = nn.Linear(user_dim, hidden_dim)\n",
    "#         self.utterance_rnn = nn.GRU(utterance_dim, hidden_dim, batch_first=True)\n",
    "#         self.exercise_embedding = nn.Linear(exercise_dim, hidden_dim)\n",
    "#         self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4)\n",
    "#         self.fc = nn.Linear(hidden_dim * 3, 4)  # 4 output classes for 4 categories\n",
    "        \n",
    "#     def forward(self, user_features, utterance_features, exercise_features):\n",
    "#         user_embedded = self.user_embedding(user_features)\n",
    "#         utterance_output, _ = self.utterance_rnn(utterance_features)\n",
    "#         exercise_embedded = self.exercise_embedding(exercise_features)\n",
    "        \n",
    "#         # Apply attention\n",
    "#         attn_output, _ = self.attention(utterance_output, utterance_output, utterance_output)\n",
    "        \n",
    "#         # Concatenate user, utterance, and exercise features\n",
    "#         combined = torch.cat([user_embedded, attn_output[:, -1, :], exercise_embedded], dim=1)\n",
    "        \n",
    "#         # Final classification\n",
    "#         output = self.fc(combined)\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         for user_features, utterance_features, exercise_features in train_loader:\n",
    "#             user_features = user_features.to(device)\n",
    "#             utterance_features = utterance_features.to(device)\n",
    "#             exercise_features = exercise_features.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(user_features, utterance_features, exercise_features)\n",
    "#             loss = criterion(outputs, exercise_features[:, 0].long())  # Assuming category is the first feature\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             train_loss += loss.item()\n",
    "        \n",
    "#         # Val\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for user_features, utterance_features, exercise_features in val_loader:\n",
    "#                 user_features = user_features.to(device)\n",
    "#                 utterance_features = utterance_features.to(device)\n",
    "#                 exercise_features = exercise_features.to(device)\n",
    "                \n",
    "#                 outputs = model(user_features, utterance_features, exercise_features)\n",
    "#                 val_loss += criterion(outputs, exercise_features[:, 0].long()).item()\n",
    "        \n",
    "#         print(f'Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset = ExerciseRecommendationDataset(users_df, utterances_df, exercises_df)\n",
    "# train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "# train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_dim = 13  # Dimension of user features\n",
    "# utterance_dim = 19  # Dimension of utterance features\n",
    "# exercise_dim = 6  # Dimension of exercise features\n",
    "# hidden_dim = 64  # Hidden dimension for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.3200, Validation Loss: 1.1624\n",
      "Epoch 2, Train Loss: 1.0614, Validation Loss: 0.9728\n",
      "Epoch 3, Train Loss: 0.8959, Validation Loss: 0.8328\n",
      "Epoch 4, Train Loss: 0.7744, Validation Loss: 0.7646\n",
      "Epoch 5, Train Loss: 0.6851, Validation Loss: 0.6492\n",
      "Epoch 6, Train Loss: 0.6020, Validation Loss: 0.5800\n",
      "Epoch 7, Train Loss: 0.5236, Validation Loss: 0.5104\n",
      "Epoch 8, Train Loss: 0.4664, Validation Loss: 0.4352\n",
      "Epoch 9, Train Loss: 0.4085, Validation Loss: 0.3926\n",
      "Epoch 10, Train Loss: 0.3514, Validation Loss: 0.3358\n",
      "Epoch 11, Train Loss: 0.3112, Validation Loss: 0.2878\n",
      "Epoch 12, Train Loss: 0.2665, Validation Loss: 0.2635\n",
      "Epoch 13, Train Loss: 0.2320, Validation Loss: 0.2124\n",
      "Epoch 14, Train Loss: 0.1997, Validation Loss: 0.1892\n",
      "Epoch 15, Train Loss: 0.1707, Validation Loss: 0.1627\n",
      "Epoch 16, Train Loss: 0.1448, Validation Loss: 0.1449\n",
      "Epoch 17, Train Loss: 0.1270, Validation Loss: 0.1202\n",
      "Epoch 18, Train Loss: 0.1085, Validation Loss: 0.1028\n",
      "Epoch 19, Train Loss: 0.0954, Validation Loss: 0.0901\n",
      "Epoch 20, Train Loss: 0.0830, Validation Loss: 0.0771\n",
      "Epoch 21, Train Loss: 0.0729, Validation Loss: 0.0689\n",
      "Epoch 22, Train Loss: 0.0646, Validation Loss: 0.0607\n",
      "Epoch 23, Train Loss: 0.0576, Validation Loss: 0.0557\n",
      "Epoch 24, Train Loss: 0.0504, Validation Loss: 0.0487\n",
      "Epoch 25, Train Loss: 0.0451, Validation Loss: 0.0440\n"
     ]
    }
   ],
   "source": [
    "# model = ExerciseRecommendationModel(user_dim, utterance_dim, exercise_dim, hidden_dim)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to recommend exercise category\n",
    "# def recommend_exercise_category(model, user, recent_utterances):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     user_features = create_user_features(user).unsqueeze(0).to(device)\n",
    "#     utterance_features = torch.stack([create_utterance_features(u) for u in recent_utterances]).unsqueeze(0).to(device)\n",
    "#     dummy_exercise_features = torch.zeros(1, 6).to(device)  # Placeholder for exercise features\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         output = model(user_features, utterance_features, dummy_exercise_features)\n",
    "    \n",
    "#     category_probs = torch.softmax(output, dim=1)\n",
    "#     recommended_category = torch.argmax(category_probs).item()\n",
    "    \n",
    "#     categories = ['grammar', 'vocabulary', 'pronunciation', 'fluency']\n",
    "#     return categories[recommended_category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended exercise category for user e0fa472b-9738-4814-b366-5e0f6f668547: grammar\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# user_id = users_df['user_id'].iloc[0]\n",
    "# user = users_df[users_df['user_id'] == user_id].iloc[0]\n",
    "# recent_utterances = utterances_df[utterances_df['user_id'] == user_id].tail(5)\n",
    "# recommended_category = recommend_exercise_category(model, user, recent_utterances.to_dict('records'))\n",
    "# print(f\"Recommended exercise category for user {user_id}: {recommended_category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL ATTENTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    users_df = pd.read_csv('users.csv')\n",
    "    utterances_df = pd.read_csv('utterances.csv')\n",
    "    errors_df = pd.read_csv('errors.csv')\n",
    "    exercises_df = pd.read_csv('exercises.csv')\n",
    "    user_progress_df = pd.read_csv('user_progress.csv')\n",
    "\n",
    "    \n",
    "    le_country = LabelEncoder()\n",
    "    le_age_band = LabelEncoder()\n",
    "    le_proficiency = LabelEncoder()\n",
    "    le_cefr = LabelEncoder()\n",
    "    le_current_focus = LabelEncoder()\n",
    "\n",
    "    users_df['country_encoded'] = le_country.fit_transform(users_df['country'])\n",
    "    users_df['age_band_encoded'] = le_age_band.fit_transform(users_df['age_band'])\n",
    "    users_df['proficiency_level_encoded'] = le_proficiency.fit_transform(users_df['proficiency_level'])\n",
    "    users_df['cefr_level_encoded'] = le_cefr.fit_transform(users_df['cefr_level'])\n",
    "    users_df['current_focus_encoded'] = le_current_focus.fit_transform(users_df['current_focus'])\n",
    "\n",
    "    users_df['interests'] = users_df['interests'].apply(eval)\n",
    "    users_df['personalized_goals'] = users_df['personalized_goals'].apply(eval)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    numerical_columns = ['overall_progress', 'total_practice_time', 'exercises_completed', 'daily_active_days', \n",
    "                         'weekly_active_days', 'total_sessions', 'avg_session_duration', 'adaptive_difficulty', \n",
    "                         'engagement_score']\n",
    "    users_df[numerical_columns] = scaler.fit_transform(users_df[numerical_columns])\n",
    "\n",
    "    \n",
    "    utterance_scaler = StandardScaler()\n",
    "    utterance_num_columns = ['grammar_overall_score', 'grammar_error_count', 'vocabulary_overall_score', 'unique_words_count',\n",
    "                             'advanced_words_count', 'lexical_density', 'pronunciation_overall_score', 'phoneme_error_count',\n",
    "                             'intonation_score', 'rhythm_score', 'accent_similarity', 'fluency_overall_score', 'words_per_minute',\n",
    "                             'filler_word_count', 'pause_count', 'avg_pause_duration', 'longest_fluent_segment', 'coherence_score',\n",
    "                             'topic_relevance_score']\n",
    "    utterances_df[utterance_num_columns] = utterance_scaler.fit_transform(utterances_df[utterance_num_columns])\n",
    "\n",
    "    \n",
    "    le_category = LabelEncoder()\n",
    "    le_subcategory = LabelEncoder()\n",
    "    le_format = LabelEncoder()\n",
    "    le_content_theme = LabelEncoder()\n",
    "\n",
    "    exercises_df['category_encoded'] = le_category.fit_transform(exercises_df['category'])\n",
    "    exercises_df['subcategory_encoded'] = le_subcategory.fit_transform(exercises_df['subcategory'])\n",
    "    exercises_df['format_encoded'] = le_format.fit_transform(exercises_df['format'])\n",
    "    exercises_df['content_theme_encoded'] = le_content_theme.fit_transform(exercises_df['content_theme'])\n",
    "\n",
    "    exercise_scaler = StandardScaler()\n",
    "    exercise_num_columns = ['difficulty', 'performance_score', 'time_taken']\n",
    "    exercises_df[exercise_num_columns] = exercise_scaler.fit_transform(exercises_df[exercise_num_columns])\n",
    "\n",
    "    return users_df, utterances_df, errors_df, exercises_df, user_progress_df, le_category, le_format, le_content_theme\n",
    "\n",
    "\n",
    "class CombinedExerciseRecommendationModel(nn.Module):\n",
    "    def __init__(self, user_dim, utterance_dim, exercise_dim, hidden_dim, num_categories, num_formats, num_themes):\n",
    "        super(CombinedExerciseRecommendationModel, self).__init__()\n",
    "        self.user_embedding = nn.Linear(user_dim, hidden_dim)\n",
    "        self.utterance_rnn = nn.GRU(utterance_dim, hidden_dim, batch_first=True)\n",
    "        self.exercise_embedding = nn.Linear(exercise_dim, hidden_dim)\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4)\n",
    "        self.fc_category = nn.Linear(hidden_dim * 3, num_categories)\n",
    "        self.fc_format = nn.Linear(hidden_dim * 3, num_formats)\n",
    "        self.fc_theme = nn.Linear(hidden_dim * 3, num_themes)\n",
    "        \n",
    "    def forward(self, user_features, utterance_features, exercise_features):\n",
    "        user_embedded = self.user_embedding(user_features)\n",
    "        utterance_output, _ = self.utterance_rnn(utterance_features)\n",
    "        exercise_embedded = self.exercise_embedding(exercise_features)\n",
    "        \n",
    "        attn_output, _ = self.attention(utterance_output, utterance_output, utterance_output)\n",
    "        \n",
    "        combined = torch.cat([user_embedded, attn_output[:, -1, :], exercise_embedded], dim=1)\n",
    "        \n",
    "        category_output = self.fc_category(combined)\n",
    "        format_output = self.fc_format(combined)\n",
    "        theme_output = self.fc_theme(combined)\n",
    "        \n",
    "        return category_output, format_output, theme_output\n",
    "\n",
    "# class for datset\n",
    "class ExerciseRecommendationDataset(Dataset):\n",
    "    def __init__(self, users, utterances, exercises, max_utterances=10):\n",
    "        self.users = users\n",
    "        self.utterances = utterances\n",
    "        self.exercises = exercises\n",
    "        self.max_utterances = max_utterances\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.exercises)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        exercise = self.exercises.iloc[idx]\n",
    "        user_id = exercise['user_id']\n",
    "        user = self.users[self.users['user_id'] == user_id].iloc[0]\n",
    "        \n",
    "        user_features = torch.tensor([\n",
    "            user['country_encoded'], user['age_band_encoded'], user['proficiency_level_encoded'],\n",
    "            user['cefr_level_encoded'], user['current_focus_encoded'], user['overall_progress'],\n",
    "            user['total_practice_time'], user['exercises_completed'], user['daily_active_days'],\n",
    "            user['weekly_active_days'], user['total_sessions'], user['avg_session_duration'],\n",
    "            user['adaptive_difficulty'], user['engagement_score']\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        user_utterances = self.utterances[self.utterances['user_id'] == user_id].tail(self.max_utterances)\n",
    "        utterance_features = torch.tensor(user_utterances[['grammar_overall_score', 'grammar_error_count',\n",
    "                                                           'vocabulary_overall_score', 'unique_words_count',\n",
    "                                                           'advanced_words_count', 'lexical_density',\n",
    "                                                           'pronunciation_overall_score', 'phoneme_error_count',\n",
    "                                                           'intonation_score', 'rhythm_score', 'accent_similarity',\n",
    "                                                           'fluency_overall_score', 'words_per_minute',\n",
    "                                                           'filler_word_count', 'pause_count', 'avg_pause_duration',\n",
    "                                                           'longest_fluent_segment', 'coherence_score',\n",
    "                                                           'topic_relevance_score']].values, dtype=torch.float32)\n",
    "        \n",
    "        if len(utterance_features) < self.max_utterances:\n",
    "            padding = torch.zeros((self.max_utterances - len(utterance_features), utterance_features.size(1)))\n",
    "            utterance_features = torch.cat([utterance_features, padding], dim=0)\n",
    "        \n",
    "        exercise_features = torch.tensor([\n",
    "            exercise['category_encoded'], exercise['subcategory_encoded'], exercise['format_encoded'],\n",
    "            exercise['content_theme_encoded'], exercise['difficulty'], exercise['performance_score'],\n",
    "            exercise['time_taken']\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        return user_features, utterance_features, exercise_features, exercise['category_encoded'], exercise['format_encoded'], exercise['content_theme_encoded']\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for user_features, utterance_features, exercise_features, category, format_, theme in train_loader:\n",
    "            user_features = user_features.to(device).float()\n",
    "            utterance_features = utterance_features.to(device).float()\n",
    "            exercise_features = exercise_features.to(device).float()\n",
    "            category = category.to(device, dtype=torch.long)\n",
    "            format_ = format_.to(device, dtype=torch.long)\n",
    "            theme = theme.to(device, dtype=torch.long)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            category_output, format_output, theme_output = model(user_features, utterance_features, exercise_features)\n",
    "            \n",
    "            loss = criterion(category_output, category) + criterion(format_output, format_) + criterion(theme_output, theme)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for user_features, utterance_features, exercise_features, category, format_, theme in val_loader:\n",
    "                user_features = user_features.to(device).float()\n",
    "                utterance_features = utterance_features.to(device).float()\n",
    "                exercise_features = exercise_features.to(device).float()\n",
    "                category = category.to(device ,dtype=torch.long)\n",
    "                format_ = format_.to(device ,dtype=torch.long)\n",
    "                theme = theme.to(device ,dtype=torch.long)\n",
    "                \n",
    "                category_output, format_output, theme_output = model(user_features, utterance_features, exercise_features)\n",
    "                loss = criterion(category_output, category) + criterion(format_output, format_) + criterion(theme_output, theme)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "# recommendation function\n",
    "def recommend_exercise(model, user, recent_utterances, users_df, utterances_df, le_category, le_format, le_content_theme, device):\n",
    "    model.eval()\n",
    "    \n",
    "    user_features = torch.tensor([\n",
    "        user['country_encoded'], user['age_band_encoded'], user['proficiency_level_encoded'],\n",
    "        user['cefr_level_encoded'], user['current_focus_encoded'], user['overall_progress'],\n",
    "        user['total_practice_time'], user['exercises_completed'], user['daily_active_days'],\n",
    "        user['weekly_active_days'], user['total_sessions'], user['avg_session_duration'],\n",
    "        user['adaptive_difficulty'], user['engagement_score']\n",
    "    ], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    utterance_features = torch.tensor(recent_utterances[['grammar_overall_score', 'grammar_error_count',\n",
    "                                                         'vocabulary_overall_score', 'unique_words_count',\n",
    "                                                         'advanced_words_count', 'lexical_density',\n",
    "                                                         'pronunciation_overall_score', 'phoneme_error_count',\n",
    "                                                         'intonation_score', 'rhythm_score', 'accent_similarity',\n",
    "                                                         'fluency_overall_score', 'words_per_minute',\n",
    "                                                         'filler_word_count', 'pause_count', 'avg_pause_duration',\n",
    "                                                         'longest_fluent_segment', 'coherence_score',\n",
    "                                                         'topic_relevance_score']].values, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    dummy_exercise_features = torch.zeros(1, 7).to(device)  # Placeholder for exercise features\n",
    "\n",
    "    with torch.no_grad():\n",
    "        category_output, format_output, theme_output = model(user_features, utterance_features, dummy_exercise_features)\n",
    "\n",
    "    category_probs = torch.softmax(category_output, dim=1)\n",
    "    format_probs = torch.softmax(format_output, dim=1)\n",
    "    theme_probs = torch.softmax(theme_output, dim=1)\n",
    "\n",
    "    recommended_category = le_category.inverse_transform([torch.argmax(category_probs).item()])[0]\n",
    "    recommended_format = le_format.inverse_transform([torch.argmax(format_probs).item()])[0]\n",
    "    recommended_theme = le_content_theme.inverse_transform([torch.argmax(theme_probs).item()])[0]\n",
    "\n",
    "\n",
    "    user_interests = user['interests']\n",
    "    user_country = user['country']\n",
    "    \n",
    "    content_theme = np.random.choice(user_interests) if user_interests else recommended_theme\n",
    "    \n",
    "    # Simple logic for exercise format can be modified in diffrent ways- simple for brevity\n",
    "    formats = [\"fill-in-the-blank\", \"multiple-choice\", \"audio-recording\", \"sentence-formation\"]\n",
    "    format_weights = [0.3, 0.3, 0.2, 0.2]\n",
    "    if recommended_category == 'pronunciation':\n",
    "        format_weights = [0.1, 0.1, 0.7, 0.1]\n",
    "    elif recommended_category == 'grammar':\n",
    "        format_weights = [0.4, 0.4, 0.1, 0.1]\n",
    "    \n",
    "    exercise_format = np.random.choice(formats, p=format_weights)\n",
    "    \n",
    "    return {\n",
    "        'category': recommended_category,\n",
    "        'content_theme': content_theme,\n",
    "        'format': exercise_format\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 4.3742, Validation Loss: 3.7487\n",
      "Epoch 2, Train Loss: 3.3947, Validation Loss: 3.1228\n",
      "Epoch 3, Train Loss: 2.8695, Validation Loss: 2.6901\n",
      "Epoch 4, Train Loss: 2.4918, Validation Loss: 2.4000\n",
      "Epoch 5, Train Loss: 2.2527, Validation Loss: 2.2254\n",
      "Epoch 6, Train Loss: 2.0014, Validation Loss: 1.9935\n",
      "Epoch 7, Train Loss: 1.7950, Validation Loss: 1.7999\n",
      "Epoch 8, Train Loss: 1.5702, Validation Loss: 1.5419\n",
      "Epoch 9, Train Loss: 1.3904, Validation Loss: 1.4232\n",
      "Epoch 10, Train Loss: 1.2341, Validation Loss: 1.1793\n",
      "Epoch 11, Train Loss: 1.0937, Validation Loss: 1.0261\n",
      "Epoch 12, Train Loss: 0.9706, Validation Loss: 1.0047\n",
      "Epoch 13, Train Loss: 0.8884, Validation Loss: 0.9607\n",
      "Epoch 14, Train Loss: 0.7687, Validation Loss: 0.8299\n",
      "Epoch 15, Train Loss: 0.7039, Validation Loss: 0.7444\n",
      "Epoch 16, Train Loss: 0.6296, Validation Loss: 0.6435\n",
      "Epoch 17, Train Loss: 0.5751, Validation Loss: 0.6613\n",
      "Epoch 18, Train Loss: 0.5355, Validation Loss: 0.6391\n",
      "Epoch 19, Train Loss: 0.5131, Validation Loss: 0.4926\n",
      "Epoch 20, Train Loss: 0.4363, Validation Loss: 0.4682\n",
      "Epoch 21, Train Loss: 0.4143, Validation Loss: 0.4764\n",
      "Epoch 22, Train Loss: 0.4147, Validation Loss: 0.4727\n",
      "Epoch 23, Train Loss: 0.3796, Validation Loss: 0.4738\n",
      "Epoch 24, Train Loss: 0.4038, Validation Loss: 0.4166\n",
      "Epoch 25, Train Loss: 0.3371, Validation Loss: 0.3723\n",
      "Epoch 26, Train Loss: 0.3187, Validation Loss: 0.3343\n",
      "Epoch 27, Train Loss: 0.2829, Validation Loss: 0.3054\n",
      "Epoch 28, Train Loss: 0.2733, Validation Loss: 0.3228\n",
      "Epoch 29, Train Loss: 0.2652, Validation Loss: 0.2897\n",
      "Epoch 30, Train Loss: 0.2534, Validation Loss: 0.3158\n",
      "Epoch 31, Train Loss: 0.2428, Validation Loss: 0.2880\n",
      "Epoch 32, Train Loss: 0.2242, Validation Loss: 0.2604\n",
      "Epoch 33, Train Loss: 0.2151, Validation Loss: 0.2464\n",
      "Epoch 34, Train Loss: 0.2120, Validation Loss: 0.2326\n",
      "Epoch 35, Train Loss: 0.2079, Validation Loss: 0.2634\n",
      "Epoch 36, Train Loss: 0.2145, Validation Loss: 0.3075\n",
      "Epoch 37, Train Loss: 0.1893, Validation Loss: 0.2086\n",
      "Epoch 38, Train Loss: 0.1682, Validation Loss: 0.2289\n",
      "Epoch 39, Train Loss: 0.1764, Validation Loss: 0.2308\n",
      "Epoch 40, Train Loss: 0.1593, Validation Loss: 0.1836\n",
      "Epoch 41, Train Loss: 0.1594, Validation Loss: 0.1731\n",
      "Epoch 42, Train Loss: 0.1569, Validation Loss: 0.2126\n",
      "Epoch 43, Train Loss: 0.1404, Validation Loss: 0.1969\n",
      "Epoch 44, Train Loss: 0.1445, Validation Loss: 0.1906\n",
      "Epoch 45, Train Loss: 0.1479, Validation Loss: 0.1837\n",
      "Epoch 46, Train Loss: 0.1310, Validation Loss: 0.1635\n",
      "Epoch 47, Train Loss: 0.1456, Validation Loss: 0.2308\n",
      "Epoch 48, Train Loss: 0.1229, Validation Loss: 0.1460\n",
      "Epoch 49, Train Loss: 0.1222, Validation Loss: 0.1561\n",
      "Epoch 50, Train Loss: 0.1231, Validation Loss: 0.1500\n",
      "Epoch 51, Train Loss: 0.1258, Validation Loss: 0.2043\n",
      "Epoch 52, Train Loss: 0.1359, Validation Loss: 0.1481\n",
      "Epoch 53, Train Loss: 0.1080, Validation Loss: 0.1568\n",
      "Epoch 54, Train Loss: 0.0966, Validation Loss: 0.1420\n",
      "Epoch 55, Train Loss: 0.0981, Validation Loss: 0.1606\n",
      "Epoch 56, Train Loss: 0.1078, Validation Loss: 0.1517\n",
      "Epoch 57, Train Loss: 0.1353, Validation Loss: 0.1789\n",
      "Epoch 58, Train Loss: 0.1054, Validation Loss: 0.1228\n",
      "Epoch 59, Train Loss: 0.0903, Validation Loss: 0.1228\n",
      "Epoch 60, Train Loss: 0.0895, Validation Loss: 0.1305\n",
      "Epoch 61, Train Loss: 0.0770, Validation Loss: 0.1156\n",
      "Epoch 62, Train Loss: 0.0825, Validation Loss: 0.1287\n",
      "Epoch 63, Train Loss: 0.0778, Validation Loss: 0.1111\n",
      "Epoch 64, Train Loss: 0.0766, Validation Loss: 0.1155\n",
      "Epoch 65, Train Loss: 0.0739, Validation Loss: 0.1027\n",
      "Epoch 66, Train Loss: 0.0668, Validation Loss: 0.1054\n",
      "Epoch 67, Train Loss: 0.0738, Validation Loss: 0.1211\n",
      "Epoch 68, Train Loss: 0.0658, Validation Loss: 0.0921\n",
      "Epoch 69, Train Loss: 0.0615, Validation Loss: 0.0926\n",
      "Epoch 70, Train Loss: 0.0630, Validation Loss: 0.1268\n",
      "Epoch 71, Train Loss: 0.0867, Validation Loss: 0.2113\n",
      "Epoch 72, Train Loss: 0.1115, Validation Loss: 0.0992\n",
      "Epoch 73, Train Loss: 0.0587, Validation Loss: 0.1090\n",
      "Epoch 74, Train Loss: 0.0566, Validation Loss: 0.0922\n",
      "Epoch 75, Train Loss: 0.0538, Validation Loss: 0.0888\n",
      "Epoch 76, Train Loss: 0.0498, Validation Loss: 0.1124\n",
      "Epoch 77, Train Loss: 0.0473, Validation Loss: 0.0983\n",
      "Epoch 78, Train Loss: 0.0551, Validation Loss: 0.1097\n",
      "Epoch 79, Train Loss: 0.0621, Validation Loss: 0.0901\n",
      "Epoch 80, Train Loss: 0.0444, Validation Loss: 0.0806\n",
      "Epoch 81, Train Loss: 0.0411, Validation Loss: 0.0850\n",
      "Epoch 82, Train Loss: 0.0398, Validation Loss: 0.0769\n",
      "Epoch 83, Train Loss: 0.0496, Validation Loss: 0.0846\n",
      "Epoch 84, Train Loss: 0.0452, Validation Loss: 0.0886\n",
      "Epoch 85, Train Loss: 0.0510, Validation Loss: 0.0709\n",
      "Epoch 86, Train Loss: 0.0378, Validation Loss: 0.0687\n",
      "Epoch 87, Train Loss: 0.0363, Validation Loss: 0.0682\n",
      "Epoch 88, Train Loss: 0.0329, Validation Loss: 0.0673\n",
      "Epoch 89, Train Loss: 0.0298, Validation Loss: 0.0778\n",
      "Epoch 90, Train Loss: 0.0302, Validation Loss: 0.0729\n",
      "Epoch 91, Train Loss: 0.0381, Validation Loss: 0.0671\n",
      "Epoch 92, Train Loss: 0.0428, Validation Loss: 0.0589\n",
      "Epoch 93, Train Loss: 0.0409, Validation Loss: 0.0890\n",
      "Epoch 94, Train Loss: 0.2140, Validation Loss: 0.0973\n",
      "Epoch 95, Train Loss: 0.1089, Validation Loss: 0.1447\n",
      "Epoch 96, Train Loss: 0.0416, Validation Loss: 0.0700\n",
      "Epoch 97, Train Loss: 0.0379, Validation Loss: 0.0552\n",
      "Epoch 98, Train Loss: 0.0318, Validation Loss: 0.0509\n",
      "Epoch 99, Train Loss: 0.0316, Validation Loss: 0.0457\n",
      "Epoch 100, Train Loss: 0.0248, Validation Loss: 0.0484\n",
      "Recommended exercise for user e0fa472b-9738-4814-b366-5e0f6f668547:\n",
      "{'category': 'fluency', 'content_theme': 'anime', 'format': 'multiple-choice'}\n",
      "Simulating session for user 5b9e5dbd-7ff6-4cd2-951a-66bad4330bed\n",
      "\n",
      "Exercise 1\n",
      "Recommending exercise:\n",
      "{'category': 'fluency', 'content_theme': 'cooking', 'format': 'fill-in-the-blank'}\n",
      "Simulated performance score: 0.68\n",
      "Simulated time taken: 183 seconds\n",
      "\n",
      "Exercise 2\n",
      "Recommending exercise:\n",
      "{'category': 'fluency', 'content_theme': 'movies', 'format': 'fill-in-the-blank'}\n",
      "Simulated performance score: 0.96\n",
      "Simulated time taken: 225 seconds\n",
      "\n",
      "Exercise 3\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'cooking', 'format': 'multiple-choice'}\n",
      "Simulated performance score: 0.88\n",
      "Simulated time taken: 278 seconds\n",
      "\n",
      "Exercise 4\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'cooking', 'format': 'fill-in-the-blank'}\n",
      "Simulated performance score: 0.93\n",
      "Simulated time taken: 148 seconds\n",
      "\n",
      "Exercise 5\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'technology', 'format': 'sentence-formation'}\n",
      "Simulated performance score: 0.77\n",
      "Simulated time taken: 73 seconds\n",
      "\n",
      "Exercise 6\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'movies', 'format': 'multiple-choice'}\n",
      "Simulated performance score: 0.76\n",
      "Simulated time taken: 174 seconds\n",
      "\n",
      "Exercise 7\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'movies', 'format': 'multiple-choice'}\n",
      "Simulated performance score: 0.71\n",
      "Simulated time taken: 214 seconds\n",
      "\n",
      "Exercise 8\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'movies', 'format': 'multiple-choice'}\n",
      "Simulated performance score: 0.60\n",
      "Simulated time taken: 274 seconds\n",
      "\n",
      "Exercise 9\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'movies', 'format': 'multiple-choice'}\n",
      "Simulated performance score: 0.56\n",
      "Simulated time taken: 200 seconds\n",
      "\n",
      "Exercise 10\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'technology', 'format': 'audio-recording'}\n",
      "Simulated performance score: 0.79\n",
      "Simulated time taken: 218 seconds\n",
      "\n",
      "Exercise 11\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'technology', 'format': 'fill-in-the-blank'}\n",
      "Simulated performance score: 0.73\n",
      "Simulated time taken: 81 seconds\n",
      "\n",
      "Exercise 12\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'cooking', 'format': 'multiple-choice'}\n",
      "Simulated performance score: 0.76\n",
      "Simulated time taken: 157 seconds\n",
      "\n",
      "Exercise 13\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'movies', 'format': 'fill-in-the-blank'}\n",
      "Simulated performance score: 0.65\n",
      "Simulated time taken: 90 seconds\n",
      "\n",
      "Exercise 14\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'cooking', 'format': 'multiple-choice'}\n",
      "Simulated performance score: 0.67\n",
      "Simulated time taken: 123 seconds\n",
      "\n",
      "Exercise 15\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'cooking', 'format': 'sentence-formation'}\n",
      "Simulated performance score: 0.92\n",
      "Simulated time taken: 189 seconds\n",
      "\n",
      "Exercise 16\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'movies', 'format': 'multiple-choice'}\n",
      "Simulated performance score: 0.58\n",
      "Simulated time taken: 234 seconds\n",
      "\n",
      "Exercise 17\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'technology', 'format': 'fill-in-the-blank'}\n",
      "Simulated performance score: 0.53\n",
      "Simulated time taken: 140 seconds\n",
      "\n",
      "Exercise 18\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'cooking', 'format': 'audio-recording'}\n",
      "Simulated performance score: 0.61\n",
      "Simulated time taken: 277 seconds\n",
      "\n",
      "Exercise 19\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'movies', 'format': 'fill-in-the-blank'}\n",
      "Simulated performance score: 0.57\n",
      "Simulated time taken: 173 seconds\n",
      "\n",
      "Exercise 20\n",
      "Recommending exercise:\n",
      "{'category': 'grammar', 'content_theme': 'technology', 'format': 'sentence-formation'}\n",
      "Simulated performance score: 0.57\n",
      "Simulated time taken: 208 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    users_df, utterances_df, errors_df, exercises_df, user_progress_df, le_category, le_format, le_content_theme = load_and_preprocess_data()\n",
    "    \n",
    "    dataset = ExerciseRecommendationDataset(users_df, utterances_df, exercises_df)\n",
    "    train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "    user_dim = 14  # Dimension of user features\n",
    "    utterance_dim = 19  # Dimension of utterance features\n",
    "    exercise_dim = 7  # Dimension of exercise features\n",
    "    hidden_dim = 128  # Hidden dimension for the model\n",
    "    num_categories = len(le_category.classes_)\n",
    "    num_formats = len(le_format.classes_)\n",
    "    num_themes = len(le_content_theme.classes_)\n",
    "\n",
    "    model = CombinedExerciseRecommendationModel(user_dim, utterance_dim, exercise_dim, hidden_dim, num_categories, num_formats, num_themes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, device=device)\n",
    "\n",
    "    \n",
    "    user_id = users_df['user_id'].iloc[0]\n",
    "    user = users_df[users_df['user_id'] == user_id].iloc[0]\n",
    "    recent_utterances = utterances_df[utterances_df['user_id'] == user_id].tail(10)\n",
    "\n",
    "    recommendation = recommend_exercise(model, user, recent_utterances, users_df, utterances_df, le_category, le_format, le_content_theme, device)\n",
    "    print(f\"Recommended exercise for user {user_id}:\")\n",
    "    print(recommendation)\n",
    "\n",
    "   \n",
    "    def update_user_progress(user_id, exercise_performance):\n",
    "        user_index = users_df.index[users_df['user_id'] == user_id].tolist()[0]\n",
    "        users_df.loc[user_index, 'overall_progress'] += exercise_performance['performance_score'] * 0.01\n",
    "        users_df.loc[user_index, 'exercises_completed'] += 1\n",
    "        users_df.loc[user_index, 'total_practice_time'] += exercise_performance['time_taken']\n",
    "        users_df.loc[user_index, 'engagement_score'] += exercise_performance['performance_score'] * 0.005\n",
    "\n",
    "        # recalculate the user's features\n",
    "        user = users_df.loc[user_index]\n",
    "        return user\n",
    "\n",
    "    # simulating the recommendation system in action\n",
    "    def simulate_user_session(user_id, num_exercises=20):\n",
    "        print(f\"Simulating session for user {user_id}\")\n",
    "        user = users_df[users_df['user_id'] == user_id].iloc[0]\n",
    "        utterances_df = pd.read_csv('utterances.csv')  # Reload the utterances for simulation\n",
    "        \n",
    "        \n",
    "        for i in range(num_exercises):\n",
    "            print(f\"\\nExercise {i+1}\")\n",
    "            recent_utterances = utterances_df[utterances_df['user_id'] == user_id].tail(10)\n",
    "            recommendation = recommend_exercise(model, user, recent_utterances, users_df, utterances_df, le_category, le_format, le_content_theme, device)\n",
    "            print(\"Recommending exercise:\")\n",
    "            print(recommendation)\n",
    "            \n",
    "            # simulate exercise completion \n",
    "            performance_score = np.random.uniform(0.5, 1.0)\n",
    "            time_taken = np.random.uniform(60, 300)  # seconds\n",
    "            print(f\"Simulated performance score: {performance_score:.2f}\")\n",
    "            print(f\"Simulated time taken: {time_taken:.0f} seconds\")\n",
    "            \n",
    "            # update user progress\n",
    "            user = update_user_progress(user_id, {'performance_score': performance_score, 'time_taken': time_taken})\n",
    "            \n",
    "            # simulate a new utterance\n",
    "            new_utterance = utterances_df[utterances_df['user_id'] == user_id].sample(1).iloc[0]\n",
    "            new_utterance['timestamp'] = pd.Timestamp.now()\n",
    "            utterances_df = pd.concat([utterances_df, new_utterance], ignore_index=True)\n",
    "\n",
    "    \n",
    "    random_user_id = np.random.choice(users_df['user_id'])\n",
    "    \n",
    "    simulate_user_session(random_user_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
