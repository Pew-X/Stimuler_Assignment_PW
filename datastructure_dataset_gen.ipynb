{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "class User:\n",
    "    def __init__(self, user_id, country, age_band, proficiency_level):\n",
    "        self.user_id = user_id\n",
    "        self.country = country\n",
    "        self.age_band = age_band\n",
    "        self.proficiency_level = proficiency_level\n",
    "        self.interests = []\n",
    "        self.error_history = {\n",
    "            \"grammar\": {},\n",
    "            \"vocabulary\": {},\n",
    "            \"pronunciation\": {},\n",
    "            \"fluency\": {}\n",
    "        }\n",
    "        self.exercise_history = []\n",
    "\n",
    "class ErrorEntry:\n",
    "    def __init__(self, category, subcategory, severity, timestamp):\n",
    "        self.category = category\n",
    "        self.subcategory = subcategory\n",
    "        self.severity = severity\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "class ExerciseEntry:\n",
    "    def __init__(self, category, subcategory, format, content, user_performance, timestamp):\n",
    "        self.category = category\n",
    "        self.subcategory = subcategory\n",
    "        self.format = format\n",
    "        self.content = content\n",
    "        self.user_performance = user_performance\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "class UtteranceAnalysis:\n",
    "    def __init__(self, user_id, utterance_id, timestamp, text, audio_file):\n",
    "        self.user_id = user_id\n",
    "        self.utterance_id = utterance_id\n",
    "        self.timestamp = timestamp\n",
    "        self.text = text\n",
    "        self.audio_file = audio_file\n",
    "        self.grammar = GrammarAnalysis()\n",
    "        self.vocabulary = VocabularyAnalysis()\n",
    "        self.pronunciation = PronunciationAnalysis()\n",
    "        self.fluency = FluencyAnalysis()\n",
    "\n",
    "class GrammarAnalysis:\n",
    "    def __init__(self):\n",
    "        self.overall_score = 0.0\n",
    "        self.error_count = 0\n",
    "        self.error_types = {\n",
    "            \"subject_verb_agreement\": [],\n",
    "            \"tense_usage\": [],\n",
    "            \"article_usage\": [],\n",
    "            \"preposition_usage\": [],\n",
    "            \"word_order\": [],\n",
    "            \"conjunction_usage\": [],\n",
    "            \"pronoun_usage\": [],\n",
    "            \"sentence_structure\": [],\n",
    "            \"punctuation\": []\n",
    "        }\n",
    "    \n",
    "    def add_error(self, error_type, severity, context):\n",
    "        self.error_types[error_type].append({\n",
    "            \"severity\": severity,\n",
    "            \"context\": context\n",
    "        })\n",
    "        self.error_count += 1\n",
    "\n",
    "class VocabularyAnalysis:\n",
    "    def __init__(self):\n",
    "        self.overall_score = 0.0\n",
    "        self.unique_words_count = 0\n",
    "        self.advanced_words_count = 0\n",
    "        self.word_choice_errors = []\n",
    "        self.collocations = []\n",
    "        self.idiom_usage = []\n",
    "        self.lexical_density = 0.0\n",
    "        self.cefr_level = \"\"\n",
    "\n",
    "    def add_word_choice_error(self, incorrect_word, suggested_word, context):\n",
    "        self.word_choice_errors.append({\n",
    "            \"incorrect\": incorrect_word,\n",
    "            \"suggested\": suggested_word,\n",
    "            \"context\": context\n",
    "        })\n",
    "\n",
    "class PronunciationAnalysis:\n",
    "    def __init__(self):\n",
    "        self.overall_score = 0.0\n",
    "        self.phoneme_errors = []\n",
    "        self.stress_errors = []\n",
    "        self.intonation_score = 0.0\n",
    "        self.rhythm_score = 0.0\n",
    "        self.fluency_score = 0.0\n",
    "        self.accent_similarity = \"\"\n",
    "\n",
    "    def add_phoneme_error(self, incorrect_phoneme, correct_phoneme, word):\n",
    "        self.phoneme_errors.append({\n",
    "            \"incorrect\": incorrect_phoneme,\n",
    "            \"correct\": correct_phoneme,\n",
    "            \"word\": word\n",
    "        })\n",
    "\n",
    "class FluencyAnalysis:\n",
    "    def __init__(self):\n",
    "        self.overall_score = 0.0\n",
    "        self.words_per_minute = 0\n",
    "        self.filler_word_count = 0\n",
    "        self.pause_count = 0\n",
    "        self.avg_pause_duration = 0.0\n",
    "        self.longest_fluent_segment = 0\n",
    "        self.sentence_restarts = 0\n",
    "        self.coherence_score = 0.0\n",
    "        self.topic_relevance_score = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generated and saved to separate JSON files:\n",
      "users.json, utterances.json, errors.json, exercises.json\n",
      "\n",
      "Sample User:\n",
      "{\n",
      "  \"user_id\": \"4c67606a-5443-4ed9-9257-37aa4082de07\",\n",
      "  \"country\": \"Japan\",\n",
      "  \"age_band\": \"35-44\",\n",
      "  \"proficiency_level\": \"Intermediate\",\n",
      "  \"interests\": [\n",
      "    \"anime\",\n",
      "    \"sports\",\n",
      "    \"music\",\n",
      "    \"cooking\",\n",
      "    \"literature\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Sample Utterance:\n",
      "{\n",
      "  \"utterance_id\": \"054e3c91-d6ce-4083-bac9-e90c6f87876a\",\n",
      "  \"user_id\": \"4c67606a-5443-4ed9-9257-37aa4082de07\",\n",
      "  \"timestamp\": \"2024-09-26T06:30:04.585674\",\n",
      "  \"text\": \"Sample text for utterance 0\",\n",
      "  \"audio_file\": \"audio_054e3c91-d6ce-4083-bac9-e90c6f87876a.wav\"\n",
      "}\n",
      "\n",
      "Sample Error:\n",
      "{\n",
      "  \"error_id\": \"2ac32f5d-1d43-4b49-9ca5-67c9cee95e59\",\n",
      "  \"utterance_id\": \"054e3c91-d6ce-4083-bac9-e90c6f87876a\",\n",
      "  \"user_id\": \"4c67606a-5443-4ed9-9257-37aa4082de07\",\n",
      "  \"timestamp\": \"2024-09-26T06:30:04.585674\",\n",
      "  \"category\": \"fluency\",\n",
      "  \"subcategory\": \"coherence\",\n",
      "  \"severity\": 0.46422879981624215\n",
      "}\n",
      "\n",
      "Sample Exercise:\n",
      "{\n",
      "  \"exercise_id\": \"a21f6f85-7064-4f79-87e7-4027037f03f2\",\n",
      "  \"user_id\": \"4c67606a-5443-4ed9-9257-37aa4082de07\",\n",
      "  \"timestamp\": \"2024-09-18T20:23:51.585674\",\n",
      "  \"category\": \"grammar\",\n",
      "  \"subcategory\": \"preposition_usage\",\n",
      "  \"format\": \"sentence-formation\",\n",
      "  \"content_theme\": \"sports\",\n",
      "  \"difficulty\": 0.5681081039112076,\n",
      "  \"user_performance\": 0.785644785562751\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Helper function to generate timestamps\n",
    "def random_timestamp(start, end):\n",
    "    return (start + timedelta(seconds=random.randint(0, int((end - start).total_seconds())))).isoformat()\n",
    "\n",
    "# Generate dataset\n",
    "def generate_granular_dataset(num_users=100, days_of_history=30):\n",
    "    start_date = datetime.now() - timedelta(days=days_of_history)\n",
    "    end_date = datetime.now()\n",
    "\n",
    "    users = []\n",
    "    utterances = []\n",
    "    errors = []\n",
    "    exercises = []\n",
    "\n",
    "    countries = [\"Japan\", \"India\", \"USA\", \"Brazil\", \"France\", \"Nigeria\", \"China\", \"Australia\"]\n",
    "    age_bands = [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55+\"]\n",
    "    proficiency_levels = [\"Beginner\", \"Intermediate\", \"Advanced\"]\n",
    "    interests = [\"anime\", \"technology\", \"cooking\", \"sports\", \"music\", \"travel\", \"movies\", \"literature\"]\n",
    "    \n",
    "    categories = [\"grammar\", \"vocabulary\", \"pronunciation\", \"fluency\"]\n",
    "    subcategories = {\n",
    "        \"grammar\": [\"subject_verb_agreement\", \"tense_usage\", \"article_usage\", \"preposition_usage\"],\n",
    "        \"vocabulary\": [\"word_choice\", \"idiomatic_expressions\", \"collocations\", \"academic_vocabulary\"],\n",
    "        \"pronunciation\": [\"consonant_sounds\", \"vowel_sounds\", \"word_stress\", \"intonation\"],\n",
    "        \"fluency\": [\"speaking_speed\", \"pausing\", \"coherence\", \"filler_words\"]\n",
    "    }\n",
    "    exercise_formats = [\"fill-in-the-blank\", \"multiple-choice\", \"audio-recording\", \"sentence-formation\"]\n",
    "\n",
    "    for i in range(num_users):\n",
    "        user_id = str(uuid.uuid4())\n",
    "        user = {\n",
    "            \"user_id\": user_id,\n",
    "            \"country\": random.choice(countries),\n",
    "            \"age_band\": random.choice(age_bands),\n",
    "            \"proficiency_level\": random.choice(proficiency_levels),\n",
    "            \"interests\": random.sample(interests, random.randint(2, 5))\n",
    "        }\n",
    "        users.append(user)\n",
    "\n",
    "        # Generate utterances for this user\n",
    "        for _ in range(random.randint(10, 50)):\n",
    "            utterance_id = str(uuid.uuid4())\n",
    "            timestamp = random_timestamp(start_date, end_date)\n",
    "            utterance = {\n",
    "                \"utterance_id\": utterance_id,\n",
    "                \"user_id\": user_id,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"text\": f\"Sample text for utterance {_}\",\n",
    "                \"audio_file\": f\"audio_{utterance_id}.wav\"\n",
    "            }\n",
    "            utterances.append(utterance)\n",
    "\n",
    "            # Generate errors for this utterance\n",
    "            for category in categories:\n",
    "                if random.random() < 0.3:  # 30% chance of error in each category\n",
    "                    subcategory = random.choice(subcategories[category])\n",
    "                    error = {\n",
    "                        \"error_id\": str(uuid.uuid4()),\n",
    "                        \"utterance_id\": utterance_id,\n",
    "                        \"user_id\": user_id,\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"category\": category,\n",
    "                        \"subcategory\": subcategory,\n",
    "                        \"severity\": random.uniform(0.3, 1.0)\n",
    "                    }\n",
    "                    errors.append(error)\n",
    "\n",
    "        # Generate exercises for this user\n",
    "        for _ in range(random.randint(5, 30)):\n",
    "            category = random.choice(categories)\n",
    "            subcategory = random.choice(subcategories[category])\n",
    "            exercise = {\n",
    "                \"exercise_id\": str(uuid.uuid4()),\n",
    "                \"user_id\": user_id,\n",
    "                \"timestamp\": random_timestamp(start_date, end_date),\n",
    "                \"category\": category,\n",
    "                \"subcategory\": subcategory,\n",
    "                \"format\": random.choice(exercise_formats),\n",
    "                \"content_theme\": random.choice(user[\"interests\"]),\n",
    "                \"difficulty\": random.uniform(0.1, 1.0),\n",
    "                \"user_performance\": random.uniform(0.5, 1.0)\n",
    "            }\n",
    "            exercises.append(exercise)\n",
    "\n",
    "    return {\n",
    "        \"users\": users,\n",
    "        \"utterances\": utterances,\n",
    "        \"errors\": errors,\n",
    "        \"exercises\": exercises\n",
    "    }\n",
    "\n",
    "# Generate the dataset\n",
    "dataset = generate_granular_dataset()\n",
    "\n",
    "# Save the dataset to separate JSON files\n",
    "for key in dataset:\n",
    "    with open(f'{key}.json', 'w') as f:\n",
    "        json.dump(dataset[key], f, indent=2)\n",
    "\n",
    "print(\"Dataset generated and saved to separate JSON files:\")\n",
    "print(\"users.json, utterances.json, errors.json, exercises.json\")\n",
    "\n",
    "# Print sample data\n",
    "print(\"\\nSample User:\")\n",
    "print(json.dumps(dataset[\"users\"][0], indent=2))\n",
    "print(\"\\nSample Utterance:\")\n",
    "print(json.dumps(dataset[\"utterances\"][0], indent=2))\n",
    "print(\"\\nSample Error:\")\n",
    "print(json.dumps(dataset[\"errors\"][0], indent=2))\n",
    "print(\"\\nSample Exercise:\")\n",
    "print(json.dumps(dataset[\"exercises\"][0], indent=2))\n",
    "\n",
    "# convert to csv\n",
    "\n",
    "df = pd.read_json('users.json')\n",
    "df.to_csv('users.csv', index=False)\n",
    "\n",
    "df = pd.read_json('utterances.json')\n",
    "df.to_csv('utterances.csv', index=False)\n",
    "\n",
    "df = pd.read_json('errors.json')\n",
    "df.to_csv('errors.csv', index=False)\n",
    "\n",
    "df = pd.read_json('exercises.json')\n",
    "df.to_csv('exercises.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive dataset generated and saved to separate JSON files:\n",
      "users.json, utterances.json, errors.json, exercises.json, user_progress.json\n",
      "\n",
      "Sample User:\n",
      "{\n",
      "  \"user_id\": \"e0fa472b-9738-4814-b366-5e0f6f668547\",\n",
      "  \"country\": \"France\",\n",
      "  \"age_band\": \"55+\",\n",
      "  \"proficiency_level\": \"Intermediate\",\n",
      "  \"interests\": [\n",
      "    \"cooking\",\n",
      "    \"music\",\n",
      "    \"literature\",\n",
      "    \"anime\",\n",
      "    \"movies\"\n",
      "  ],\n",
      "  \"cefr_level\": \"B1\",\n",
      "  \"overall_progress\": 0.8568946970619489,\n",
      "  \"total_practice_time\": 143,\n",
      "  \"exercises_completed\": 362,\n",
      "  \"daily_active_days\": 15,\n",
      "  \"weekly_active_days\": 6,\n",
      "  \"total_sessions\": 76,\n",
      "  \"avg_session_duration\": 12,\n",
      "  \"current_focus\": \"fluency\",\n",
      "  \"adaptive_difficulty\": 0.8417098175214394,\n",
      "  \"personalized_goals\": [\n",
      "    \"pronunciation\",\n",
      "    \"fluency\",\n",
      "    \"vocabulary\"\n",
      "  ],\n",
      "  \"engagement_score\": 0.7072959300474474\n",
      "}\n",
      "\n",
      "Sample Utterance:\n",
      "{\n",
      "  \"utterance_id\": \"8d782f91-9708-4bb0-8bc7-479510f27f89\",\n",
      "  \"user_id\": \"e0fa472b-9738-4814-b366-5e0f6f668547\",\n",
      "  \"timestamp\": \"2024-09-20T21:10:40.351897\",\n",
      "  \"uttered_text\": \"Sample text for utterance 0\",\n",
      "  \"audio_file\": \"audio_8d782f91-9708-4bb0-8bc7-479510f27f89.wav\",\n",
      "  \"grammar_overall_score\": 0.19290462260048458,\n",
      "  \"grammar_error_count\": 5,\n",
      "  \"grammar_error_types\": [\n",
      "    \"subject_verb_agreement\",\n",
      "    \"preposition_usage\"\n",
      "  ],\n",
      "  \"vocabulary_overall_score\": 0.19396468162096592,\n",
      "  \"unique_words_count\": 8,\n",
      "  \"advanced_words_count\": 1,\n",
      "  \"lexical_density\": 0.5335209043380451,\n",
      "  \"pronunciation_overall_score\": 0.9580915330054034,\n",
      "  \"phoneme_error_count\": 0,\n",
      "  \"intonation_score\": 0.5644361608112664,\n",
      "  \"rhythm_score\": 0.4910098259649668,\n",
      "  \"accent_similarity\": 0.08102533766227915,\n",
      "  \"fluency_overall_score\": 0.7028176776328455,\n",
      "  \"words_per_minute\": 106,\n",
      "  \"filler_word_count\": 3,\n",
      "  \"pause_count\": 0,\n",
      "  \"avg_pause_duration\": 1.8073354461288036,\n",
      "  \"longest_fluent_segment\": 21,\n",
      "  \"coherence_score\": 0.25893858078308574,\n",
      "  \"topic_relevance_score\": 0.4535907639413179\n",
      "}\n",
      "\n",
      "Sample Error:\n",
      "{\n",
      "  \"error_id\": \"c78ececa-2dec-4828-a375-f53f882aac4d\",\n",
      "  \"utterance_id\": \"8d782f91-9708-4bb0-8bc7-479510f27f89\",\n",
      "  \"user_id\": \"e0fa472b-9738-4814-b366-5e0f6f668547\",\n",
      "  \"timestamp\": \"2024-09-20T21:10:40.351897\",\n",
      "  \"category\": \"vocabulary\",\n",
      "  \"subcategory\": \"academic_vocabulary\",\n",
      "  \"severity\": 0.9191527546421998\n",
      "}\n",
      "\n",
      "Sample Exercise:\n",
      "{\n",
      "  \"exercise_id\": \"465c71b1-5393-4d55-8d20-30cc08c50d74\",\n",
      "  \"user_id\": \"e0fa472b-9738-4814-b366-5e0f6f668547\",\n",
      "  \"timestamp\": \"2024-09-09T19:28:57.351897\",\n",
      "  \"category\": \"vocabulary\",\n",
      "  \"subcategory\": \"collocations\",\n",
      "  \"format\": \"fill-in-the-blank\",\n",
      "  \"content_theme\": \"music\",\n",
      "  \"difficulty\": 0.8935532655399839,\n",
      "  \"performance_score\": 0.6509647809669505,\n",
      "  \"time_taken\": 45\n",
      "}\n",
      "\n",
      "Sample User Progress:\n",
      "{\n",
      "  \"user_id\": \"e0fa472b-9738-4814-b366-5e0f6f668547\",\n",
      "  \"timestamp\": \"2024-10-04T19:59:15.351897\",\n",
      "  \"recent_grammar_errors\": [\n",
      "    \"article_usage\"\n",
      "  ],\n",
      "  \"vocab_improvement\": 0.066282096783043,\n",
      "  \"fluency_trend\": 0.020405328822875865,\n",
      "  \"engagement_score\": 0.53021159252022\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "def random_timestamp(start, end):\n",
    "    return (start + timedelta(seconds=random.randint(0, int((end - start).total_seconds())))).isoformat()\n",
    "\n",
    "def generate_comprehensive_dataset(num_users=100, days_of_history=30):\n",
    "    start_date = datetime.now() - timedelta(days=days_of_history)\n",
    "    end_date = datetime.now()\n",
    "\n",
    "    users = []\n",
    "    utterances = []\n",
    "    errors = []\n",
    "    exercises = []\n",
    "    user_progress = []\n",
    "\n",
    "    countries = [\"Japan\", \"India\", \"USA\", \"Brazil\", \"France\", \"Nigeria\", \"China\", \"Australia\"]\n",
    "    age_bands = [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55+\"]\n",
    "    proficiency_levels = [\"Beginner\", \"Intermediate\", \"Advanced\"]\n",
    "    interests = [\"anime\", \"technology\", \"cooking\", \"sports\", \"music\", \"travel\", \"movies\", \"literature\"]\n",
    "    \n",
    "    categories = [\"grammar\", \"vocabulary\", \"pronunciation\", \"fluency\"]\n",
    "    subcategories = {\n",
    "        \"grammar\": [\"subject_verb_agreement\", \"tense_usage\", \"article_usage\", \"preposition_usage\"],\n",
    "        \"vocabulary\": [\"word_choice\", \"idiomatic_expressions\", \"collocations\", \"academic_vocabulary\"],\n",
    "        \"pronunciation\": [\"consonant_sounds\", \"vowel_sounds\", \"word_stress\", \"intonation\"],\n",
    "        \"fluency\": [\"speaking_speed\", \"pausing\", \"coherence\", \"filler_words\"]\n",
    "    }\n",
    "    exercise_formats = [\"fill-in-the-blank\", \"multiple-choice\", \"audio-recording\", \"sentence-formation\"]\n",
    "    cefr_levels = [\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"]\n",
    "\n",
    "    for i in range(num_users):\n",
    "        user_id = str(uuid.uuid4())\n",
    "        user = {\n",
    "            \"user_id\": user_id,\n",
    "            \"country\": random.choice(countries),\n",
    "            \"age_band\": random.choice(age_bands),\n",
    "            \"proficiency_level\": random.choice(proficiency_levels),\n",
    "            \"interests\": random.sample(interests, random.randint(2, 5)),\n",
    "            \"cefr_level\": random.choice(cefr_levels),\n",
    "            \"overall_progress\": random.uniform(0, 1),\n",
    "            \"total_practice_time\": random.randint(0, 100 * days_of_history),\n",
    "            \"exercises_completed\": random.randint(0, 20 * days_of_history),\n",
    "            \"daily_active_days\": random.randint(0, days_of_history),\n",
    "            \"weekly_active_days\": random.randint(0, min(7, days_of_history)),\n",
    "            \"total_sessions\": random.randint(0, 3 * days_of_history),\n",
    "            \"avg_session_duration\": random.randint(5, 30),\n",
    "            \"current_focus\": random.choice(categories),\n",
    "            \"adaptive_difficulty\": random.uniform(0.1, 1.0),\n",
    "            \"personalized_goals\": random.sample(categories, random.randint(1, 3)),\n",
    "            \"engagement_score\": random.uniform(0, 1)\n",
    "        }\n",
    "        users.append(user)\n",
    "\n",
    "        # Generate utterances for this user\n",
    "        for _ in range(random.randint(10, 50)):\n",
    "            utterance_id = str(uuid.uuid4())\n",
    "            timestamp = random_timestamp(start_date, end_date)\n",
    "            utterance = {\n",
    "                \"utterance_id\": utterance_id,\n",
    "                \"user_id\": user_id,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"uttered_text\": f\"Sample text for utterance {_}\",\n",
    "                \"audio_file\": f\"audio_{utterance_id}.wav\",\n",
    "                \"grammar_overall_score\": random.uniform(0, 1),\n",
    "                \"grammar_error_count\": random.randint(0, 5),\n",
    "                \"grammar_error_types\": random.sample(subcategories[\"grammar\"], random.randint(0, 2)),\n",
    "                \"vocabulary_overall_score\": random.uniform(0, 1),\n",
    "                \"unique_words_count\": random.randint(5, 20),\n",
    "                \"advanced_words_count\": random.randint(0, 5),\n",
    "                \"lexical_density\": random.uniform(0.4, 0.8),\n",
    "                \"pronunciation_overall_score\": random.uniform(0, 1),\n",
    "                \"phoneme_error_count\": random.randint(0, 5),\n",
    "                \"intonation_score\": random.uniform(0, 1),\n",
    "                \"rhythm_score\": random.uniform(0, 1),\n",
    "                \"accent_similarity\": random.uniform(0, 1),\n",
    "                \"fluency_overall_score\": random.uniform(0, 1),\n",
    "                \"words_per_minute\": random.randint(60, 180),\n",
    "                \"filler_word_count\": random.randint(0, 10),\n",
    "                \"pause_count\": random.randint(0, 5),\n",
    "                \"avg_pause_duration\": random.uniform(0.2, 2.0),\n",
    "                \"longest_fluent_segment\": random.randint(5, 30),\n",
    "                \"coherence_score\": random.uniform(0, 1),\n",
    "                \"topic_relevance_score\": random.uniform(0, 1)\n",
    "            }\n",
    "            utterances.append(utterance)\n",
    "\n",
    "            # Generate errors for this utterance\n",
    "            for category in categories:\n",
    "                if random.random() < 0.3:  # 30% chance of error in each category\n",
    "                    subcategory = random.choice(subcategories[category])\n",
    "                    error = {\n",
    "                        \"error_id\": str(uuid.uuid4()),\n",
    "                        \"utterance_id\": utterance_id,\n",
    "                        \"user_id\": user_id,\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"category\": category,\n",
    "                        \"subcategory\": subcategory,\n",
    "                        \"severity\": random.uniform(0.3, 1.0)\n",
    "                    }\n",
    "                    errors.append(error)\n",
    "\n",
    "        # Generate exercises for this user\n",
    "        for _ in range(random.randint(5, 30)):\n",
    "            exercise_id = str(uuid.uuid4())\n",
    "            category = random.choice(categories)\n",
    "            subcategory = random.choice(subcategories[category])\n",
    "            exercise = {\n",
    "                \"exercise_id\": exercise_id,\n",
    "                \"user_id\": user_id,\n",
    "                \"timestamp\": random_timestamp(start_date, end_date),\n",
    "                \"category\": category,\n",
    "                \"subcategory\": subcategory,\n",
    "                \"format\": random.choice(exercise_formats),\n",
    "                \"content_theme\": random.choice(user[\"interests\"]),\n",
    "                \"difficulty\": random.uniform(0.1, 1.0),\n",
    "                \"performance_score\": random.uniform(0.5, 1.0),\n",
    "                \"time_taken\": random.randint(30, 300)\n",
    "            }\n",
    "            exercises.append(exercise)\n",
    "\n",
    "        # Generate user progress\n",
    "        progress = {\n",
    "            \"user_id\": user_id,\n",
    "            \"timestamp\": end_date.isoformat(),\n",
    "            \"recent_grammar_errors\": random.sample(subcategories[\"grammar\"], random.randint(0, 2)),\n",
    "            \"vocab_improvement\": random.uniform(-0.1, 0.2),\n",
    "            \"fluency_trend\": random.uniform(-0.1, 0.2),\n",
    "            \"engagement_score\": random.uniform(0, 1)\n",
    "        }\n",
    "        user_progress.append(progress)\n",
    "\n",
    "    return {\n",
    "        \"users\": users,\n",
    "        \"utterances\": utterances,\n",
    "        \"errors\": errors,\n",
    "        \"exercises\": exercises,\n",
    "        \"user_progress\": user_progress\n",
    "    }\n",
    "\n",
    "# Generate the dataset\n",
    "dataset = generate_comprehensive_dataset()\n",
    "\n",
    "# Save the dataset to separate JSON files\n",
    "for key in dataset:\n",
    "    with open(f'final_data/{key}.json', 'w') as f:\n",
    "        json.dump(dataset[key], f, indent=2)\n",
    "\n",
    "print(\"Comprehensive dataset generated and saved to separate JSON files:\")\n",
    "print(\"users.json, utterances.json, errors.json, exercises.json, user_progress.json\")\n",
    "\n",
    "# Print sample data\n",
    "print(\"\\nSample User:\")\n",
    "print(json.dumps(dataset[\"users\"][0], indent=2))\n",
    "print(\"\\nSample Utterance:\")\n",
    "print(json.dumps(dataset[\"utterances\"][0], indent=2))\n",
    "print(\"\\nSample Error:\")\n",
    "print(json.dumps(dataset[\"errors\"][0], indent=2))\n",
    "print(\"\\nSample Exercise:\")\n",
    "print(json.dumps(dataset[\"exercises\"][0], indent=2))\n",
    "print(\"\\nSample User Progress:\")\n",
    "print(json.dumps(dataset[\"user_progress\"][0], indent=2))\n",
    "\n",
    "# convert to csv\n",
    "\n",
    "df = pd.read_json('final_data/users.json')\n",
    "df.to_csv('users.csv', index=False)\n",
    "\n",
    "df = pd.read_json('final_data/utterances.json')\n",
    "df.to_csv('utterances.csv', index=False)\n",
    "\n",
    "df = pd.read_json('final_data/errors.json')\n",
    "df.to_csv('errors.csv', index=False)\n",
    "\n",
    "df = pd.read_json('final_data/exercises.json')\n",
    "df.to_csv('exercises.csv', index=False)\n",
    "\n",
    "df = pd.read_json('final_data/user_progress.json')\n",
    "df.to_csv('user_progress.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
